{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decorated(message, border_char='-', padding=1, width=80):\n",
    "    border = border_char * width\n",
    "    for _ in range(padding):\n",
    "        print(border)\n",
    "    print(message.center(width))\n",
    "    for _ in range(padding):\n",
    "        print(border)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from bbox_plotter import yolo_to_corners\n",
    "from torchvision.ops import complete_box_iou_loss\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "torch.manual_seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Architecture\n",
    "Change the adaptive pooling layer according to the env (model is saved with different architectures in different systems, but the main model contains adaptive avg. pooling layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  \n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features= 128 * 8 * 8, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=4)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = self.global_avg_pool(self.pool3(torch.relu(self.conv3(x))))\n",
    "                \n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "#### CIoU for training, validation and evaluation between augmented and original unlabelled predictions\n",
    "#### Weighted CIoU for labelled and unlabelled combined samples with adaptive ramping up of weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteBoxLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompleteBoxLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred_boxes, true_boxes, reduction):\n",
    "        ciou = complete_box_iou_loss(pred_boxes, true_boxes, reduction)\n",
    "        \n",
    "        return ciou\n",
    "    \n",
    "ciou_loss_function = CompleteBoxLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCompleteBoxLoss(nn.Module):\n",
    "    def __init__(self, unlabeled_weight=1.5):\n",
    "        super(WeightedCompleteBoxLoss, self).__init__()\n",
    "        self.unlabeled_weight = unlabeled_weight\n",
    "\n",
    "    def forward(self, pred_boxes, true_boxes, is_labeled):\n",
    "        ciou = complete_box_iou_loss(pred_boxes, true_boxes)\n",
    "        labeled_loss = ciou[is_labeled == 1] \n",
    "        pseudo_labeled_loss = ciou[is_labeled == 0]\n",
    "        pseudo_labeled_loss = self.unlabeled_weight * pseudo_labeled_loss\n",
    "        total_loss = torch.cat([labeled_loss, pseudo_labeled_loss]).mean()\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel().to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, architecture, optimizer):\n",
    "    print(\"loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    \n",
    "    model = architecture()\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    model = model.to('cuda')\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Classes\n",
    "- For labelled, unlabelled and combined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_subfolder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = os.path.join(image_folder, label_subfolder) \n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image) \n",
    "\n",
    "        label_filename = os.path.splitext(self.image_files[idx])[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_folder, label_filename)\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    labels.append([class_id, x, y, w, h])\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZebraFishDataset(Dataset):\n",
    "    def __init__(self, labeled_image_folder, pseudo_labeled_image_folder, label_folder, pseudo_label_folder, transform=None):\n",
    "        self.labeled_image_folder = labeled_image_folder\n",
    "        self.pseudo_labeled_image_folder = pseudo_labeled_image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.pseudo_label_folder = pseudo_label_folder\n",
    "        self.transform = transform\n",
    "        self.labeled_images = [f for f in os.listdir(labeled_image_folder) if f.endswith('.jpg')]\n",
    "        self.pseudo_labeled_images = [f for f in os.listdir(pseudo_labeled_image_folder) if f.endswith('.jpg')]\n",
    "        self.all_images = self.labeled_images + self.pseudo_labeled_images\n",
    "        self.is_labeled = torch.tensor([1] * len(self.labeled_images) + [0] * len(self.pseudo_labeled_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.all_images[idx]\n",
    "        if self.is_labeled[idx]:\n",
    "            img_path = os.path.join(self.labeled_image_folder, img_name)\n",
    "            label_path = os.path.join(self.label_folder, img_name.rsplit(\".jpg\", 1)[0] + \".txt\")\n",
    "        else:\n",
    "            img_path = os.path.join(self.pseudo_labeled_image_folder, img_name)\n",
    "            label_path = os.path.join(self.pseudo_label_folder, img_name.rsplit(\".jpg\", 1)[0] + \".txt\")\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = f.read().strip().split()\n",
    "        label = [float(x) for x in label[1:]]  \n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, self.is_labeled[idx]\n",
    "    \n",
    "\n",
    "class ZebrafishValDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_subfolder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = os.path.join(image_folder, label_subfolder) \n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image) \n",
    "\n",
    "        label_filename = os.path.splitext(self.image_files[idx])[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_folder, label_filename)\n",
    "        label = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    label.append([x, y, w, h])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(labeled_image_folder, label_subfolder, unlabeled_image_folder):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224))\n",
    "        ])\n",
    "    labeled_dataset = LabeledDataset(labeled_image_folder, label_subfolder, transform=transform)\n",
    "    unlabeled_dataset = UnlabeledDataset(unlabeled_image_folder, transform=transform)\n",
    "    return labeled_dataset, unlabeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_image_folder = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\Labelled_images\"\n",
    "label_subfolder = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\Labelled_images\\labels\"\n",
    "unlabeled_image_folder = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\unlabelled_images\"\n",
    "pseudo_labelled_images = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\pseudo_labelled_images\"\n",
    "pseudo_labels_subfolder = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\pseudo_labelled_images\\pseudo_labels\"\n",
    "val_image_data = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\validation_data\"\n",
    "val_labels_subfolder = r\"D:\\Praharsha\\code\\CAMZ\\data\\raw\\validation_labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pseudo labels from unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_pseudo_labels(csv_filename, unlabeled_dataset, model, difference_metric=ciou_loss_function):\n",
    "    # Define augmentations\n",
    "    augmentations = [\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.GaussianBlur(3),\n",
    "        transforms.RandomSolarize(threshold=0.5, p=1.0)\n",
    "    ]\n",
    "\n",
    "    # Collect entries in a list\n",
    "    entries = []\n",
    "\n",
    "    # Process each image in the unlabeled dataset\n",
    "    for image_idx in range(len(unlabeled_dataset)):\n",
    "        predictions = []\n",
    "        augmented_predictions_list = []\n",
    "        image, file_name = unlabeled_dataset[image_idx]\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        input_image_tensor = image_tensor.unsqueeze(0)\n",
    "        input_image_tensor = input_image_tensor.to('cuda')\n",
    "        # Get the original prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            original_prediction = model(input_image_tensor)\n",
    "            predictions.append(original_prediction)\n",
    "\n",
    "        # Get predictions for augmented images\n",
    "        for i, augmentation in enumerate(augmentations):\n",
    "            augmented_image = augmentation(image)\n",
    "            augmented_image_tensor = transforms.ToTensor()(augmented_image)\n",
    "            augmented_input_image_tensor = augmented_image_tensor.unsqueeze(0)\n",
    "            augmented_input_image_tensor = augmented_input_image_tensor.to('cuda')\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                augmented_prediction = model(augmented_input_image_tensor)\n",
    "                if i == 0:\n",
    "                    augmented_prediction[0][1] = 1 - augmented_prediction[0][1]\n",
    "                augmented_predictions_list.append(augmented_prediction)\n",
    "\n",
    "        # Compute the mean of augmented predictions\n",
    "        augmented_predictions_tensor = torch.stack(augmented_predictions_list)\n",
    "        mean_augmented_prediction = augmented_predictions_tensor.mean(dim=0)\n",
    "\n",
    "        # Compute the final mean prediction\n",
    "        mean_prediction = torch.stack([original_prediction, mean_augmented_prediction]).mean(dim=0)\n",
    "\n",
    "        # Convert predictions to corners format\n",
    "        original_prediction = yolo_to_corners(original_prediction, image_height=224, image_width=224)\n",
    "        mean_augmented_prediction = yolo_to_corners(mean_augmented_prediction, image_height=224, image_width=224)\n",
    "\n",
    "        # Compute the CIoU loss\n",
    "        ciou_loss = difference_metric(original_prediction, mean_augmented_prediction, reduction='mean')\n",
    "\n",
    "        # Add the new entry to the list\n",
    "        entries.append({\n",
    "            \"filename\": file_name,\n",
    "            \"bounding_box\": mean_prediction.tolist(),\n",
    "            \"ciou_loss\": ciou_loss.item()\n",
    "        })\n",
    "\n",
    "    # Create the DataFrame once at the end\n",
    "    df = pd.DataFrame(entries)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print_decorated(f\"Generated Pseudo labels for Unlabelled Dataset ({len(unlabeled_dataset)} samples)\", border_char='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering top-K labels from ciou loss between original unlabelled predictions and augmented predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_pseudo_labels(csv_filename, pseudo_labeled_folder, pseudo_labels_folder, unlabeled_folder,  num_top=5):\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    top_df = df.sort_values(by=\"ciou_loss\").head(num_top)\n",
    "    top_files = top_df[\"filename\"].tolist()\n",
    "    top_boxes = top_df[\"bounding_box\"].apply(eval).tolist() \n",
    "    for file, box in zip(top_files, top_boxes):\n",
    "        src_path = os.path.join(unlabeled_folder, os.path.basename(file))\n",
    "        dest_path = os.path.join(pseudo_labeled_folder, os.path.basename(file))\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dest_path)\n",
    "        txt_filename = os.path.join(pseudo_labels_folder, os.path.basename(file).split(\".\")[0] + \".jpg.txt\")\n",
    "        with open(txt_filename, \"w\") as f:\n",
    "            f.write(f\"0 {round(box[0][0], 5)} {round(box[0][1], 5)} {round(box[0][2], 5)} {round(box[0][3], 5)}\\n\")\n",
    "    print_decorated(f\"Filtered Top-{num_top} Label\", border_char='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialize_writer(file_path,columns = ['epoch','loss','val_loss']):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, mode='w', newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, loss_function, epoch):\n",
    "    model.eval()\n",
    "    ciou_total_batch_loss = 0.0\n",
    "    ciou_loss_per_batch_val_history = []\n",
    "    batch_num = 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            outputs = yolo_to_corners(model(inputs), image_width=224, image_height=224)\n",
    "            norm_labels = yolo_to_corners(labels, image_width=224, image_height=224)\n",
    "            \n",
    "            ciou_loss_per_batch = loss_function(outputs, norm_labels, 'mean')\n",
    "            ciou_loss_per_batch_val_history.append([epoch+1, batch_num, ciou_loss_per_batch.item()])\n",
    "            ciou_total_batch_loss += ciou_loss_per_batch.item()\n",
    "            batch_num += 1\n",
    "        \n",
    "    ciou_total_loss = ciou_total_batch_loss / len(val_loader)\n",
    "    print(f\"Validation for Epoch [{epoch+1}], CIoU: {ciou_total_loss:.4f}\")\n",
    "    return ciou_total_loss, ciou_loss_per_batch_val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, loss_function, val_loss_function,\n",
    "          epoch_history_csv_path, batch_train_history_csv_path, batch_val_history_csv_path, \n",
    "          save_model_checkpoint_path, num_epochs=1, patience=20, delta=0.001):\n",
    "    \n",
    "    Initialize_writer(epoch_history_csv_path)\n",
    "    Initialize_writer(batch_train_history_csv_path, columns=['epoch', 'batch', 'loss'])\n",
    "    Initialize_writer(batch_val_history_csv_path, columns=['epoch', 'batch', 'loss'])\n",
    "    best_val_ciou_loss = float('inf')\n",
    "    model.train()        \n",
    "    early_stopping = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        ciou_total_batch_loss = 0.0\n",
    "        ciou_loss_per_batch_train_history = []\n",
    "        batch_num = 1\n",
    "        for inputs, labels, is_labeled in train_loader:\n",
    "            inputs = inputs.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            is_labeled = is_labeled.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            norm_labels = yolo_to_corners(labels, image_width=720, image_height=720)\n",
    "            outputs = yolo_to_corners(model(inputs), image_width=720, image_height=720)\n",
    "            ciou_loss_per_batch = loss_function(outputs, norm_labels, is_labeled)\n",
    "            ciou_loss_per_batch.backward()\n",
    "            optimizer.step()\n",
    "            ciou_loss_per_batch_train_history.append([epoch+1, batch_num, ciou_loss_per_batch.item()])\n",
    "            ciou_total_batch_loss += ciou_loss_per_batch.item()\n",
    "            batch_num+=1\n",
    "        ciou_loss_per_epoch = ciou_total_batch_loss / len(train_loader)\n",
    "        print(end='\\n')\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], CIoU: {ciou_loss_per_epoch:.4f}\", end=\" --- \")\n",
    "        print(f\"Validation for Epoch [{epoch+1}/{num_epochs}]\", end=\", \")\n",
    "        val_ciou_loss, ciou_loss_per_batch_val_history = validate(model, val_loader, val_loss_function, epoch)        \n",
    "        if (best_val_ciou_loss - val_ciou_loss) > delta:\n",
    "            best_val_ciou_loss = val_ciou_loss\n",
    "            checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "            print(\"\\n\")\n",
    "            print(f\"=========Saving Checkpoint======= at Epoch:[{epoch+1}/{num_epochs}]\", end=\"\\n\")\n",
    "            torch.save(checkpoint, save_model_checkpoint_path)\n",
    "            early_stopping = 0\n",
    "        else:\n",
    "            early_stopping += 1         \n",
    "        with open(epoch_history_csv_path, mode='a', newline=\"\") as file:\n",
    "            loss_writer = csv.writer(file)\n",
    "            loss_writer.writerow([epoch+1, ciou_loss_per_epoch, val_ciou_loss])\n",
    "        \n",
    "        with open(batch_train_history_csv_path, 'a', newline=\"\") as file:\n",
    "            loss_writer = csv.writer(file)\n",
    "            loss_writer.writerows(ciou_loss_per_batch_train_history)\n",
    "        \n",
    "        with open(batch_val_history_csv_path, 'a', newline=\"\") as file:\n",
    "            loss_writer = csv.writer(file)\n",
    "            loss_writer.writerows(ciou_loss_per_batch_val_history)\n",
    "        if early_stopping >= patience:\n",
    "            print(f\"Early stopping occurred at {epoch+1}\", end='\\n')\n",
    "            break\n",
    "    print(f\"The best Validation Loss is: {best_val_ciou_loss}\")\n",
    "    return best_val_ciou_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "                 Starting Semi-supervised Training - Round 1/23                 \n",
      "********************************************************************************\n",
      "loading checkpoint...\n",
      "................................................................................\n",
      "         Generated Pseudo labels for Unlabelled Dataset (24740 samples)         \n",
      "................................................................................\n",
      "********************************************************************************\n",
      "                            Filtered Top-1000 Label                             \n",
      "********************************************************************************\n",
      "\n",
      "Epoch [1/200], CIoU: 0.2238 --- Validation for Epoch [1/200], Validation for Epoch [1], CIoU: 0.3572\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[1/200]\n",
      "\n",
      "Epoch [2/200], CIoU: 0.2238 --- Validation for Epoch [2/200], Validation for Epoch [2], CIoU: 0.3560\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[2/200]\n",
      "\n",
      "Epoch [3/200], CIoU: 0.2238 --- Validation for Epoch [3/200], Validation for Epoch [3], CIoU: 0.3545\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[3/200]\n",
      "\n",
      "Epoch [4/200], CIoU: 0.2238 --- Validation for Epoch [4/200], Validation for Epoch [4], CIoU: 0.3547\n",
      "\n",
      "Epoch [5/200], CIoU: 0.2238 --- Validation for Epoch [5/200], Validation for Epoch [5], CIoU: 0.3597\n",
      "\n",
      "Epoch [6/200], CIoU: 0.2238 --- Validation for Epoch [6/200], Validation for Epoch [6], CIoU: 0.3565\n",
      "\n",
      "Epoch [7/200], CIoU: 0.2239 --- Validation for Epoch [7/200], Validation for Epoch [7], CIoU: 0.3553\n",
      "\n",
      "Epoch [8/200], CIoU: 0.2239 --- Validation for Epoch [8/200], Validation for Epoch [8], CIoU: 0.3571\n",
      "\n",
      "Epoch [9/200], CIoU: 0.2238 --- Validation for Epoch [9/200], Validation for Epoch [9], CIoU: 0.3562\n",
      "\n",
      "Epoch [10/200], CIoU: 0.2239 --- Validation for Epoch [10/200], Validation for Epoch [10], CIoU: 0.3579\n",
      "\n",
      "Epoch [11/200], CIoU: 0.2239 --- Validation for Epoch [11/200], Validation for Epoch [11], CIoU: 0.3583\n",
      "\n",
      "Epoch [12/200], CIoU: 0.2239 --- Validation for Epoch [12/200], Validation for Epoch [12], CIoU: 0.3608\n",
      "\n",
      "Epoch [13/200], CIoU: 0.2239 --- Validation for Epoch [13/200], Validation for Epoch [13], CIoU: 0.3567\n",
      "\n",
      "Epoch [14/200], CIoU: 0.2239 --- Validation for Epoch [14/200], Validation for Epoch [14], CIoU: 0.3588\n",
      "\n",
      "Epoch [15/200], CIoU: 0.2239 --- Validation for Epoch [15/200], Validation for Epoch [15], CIoU: 0.3566\n",
      "\n",
      "Epoch [16/200], CIoU: 0.2239 --- Validation for Epoch [16/200], Validation for Epoch [16], CIoU: 0.3556\n",
      "\n",
      "Epoch [17/200], CIoU: 0.2239 --- Validation for Epoch [17/200], Validation for Epoch [17], CIoU: 0.3588\n",
      "\n",
      "Epoch [18/200], CIoU: 0.2239 --- Validation for Epoch [18/200], Validation for Epoch [18], CIoU: 0.3563\n",
      "\n",
      "Epoch [19/200], CIoU: 0.2238 --- Validation for Epoch [19/200], Validation for Epoch [19], CIoU: 0.3560\n",
      "\n",
      "Epoch [20/200], CIoU: 0.2238 --- Validation for Epoch [20/200], Validation for Epoch [20], CIoU: 0.3566\n",
      "\n",
      "Epoch [21/200], CIoU: 0.2239 --- Validation for Epoch [21/200], Validation for Epoch [21], CIoU: 0.3553\n",
      "\n",
      "Epoch [22/200], CIoU: 0.2239 --- Validation for Epoch [22/200], Validation for Epoch [22], CIoU: 0.3566\n",
      "\n",
      "Epoch [23/200], CIoU: 0.2239 --- Validation for Epoch [23/200], Validation for Epoch [23], CIoU: 0.3569\n",
      "Early stopping occurred at 23\n",
      "The best Validation Loss is: 0.35454159788787365\n",
      "********************************************************************************\n",
      "                 Starting Semi-supervised Training - Round 2/23                 \n",
      "********************************************************************************\n",
      "loading checkpoint...\n",
      "................................................................................\n",
      "         Generated Pseudo labels for Unlabelled Dataset (23740 samples)         \n",
      "................................................................................\n",
      "********************************************************************************\n",
      "                            Filtered Top-1000 Label                             \n",
      "********************************************************************************\n",
      "\n",
      "Epoch [1/200], CIoU: 0.2765 --- Validation for Epoch [1/200], Validation for Epoch [1], CIoU: 0.3567\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[1/200]\n",
      "\n",
      "Epoch [2/200], CIoU: 0.2764 --- Validation for Epoch [2/200], Validation for Epoch [2], CIoU: 0.3598\n",
      "\n",
      "Epoch [3/200], CIoU: 0.2764 --- Validation for Epoch [3/200], Validation for Epoch [3], CIoU: 0.3564\n",
      "\n",
      "Epoch [4/200], CIoU: 0.2786 --- Validation for Epoch [4/200], Validation for Epoch [4], CIoU: 0.3578\n",
      "\n",
      "Epoch [5/200], CIoU: 0.2759 --- Validation for Epoch [5/200], Validation for Epoch [5], CIoU: 0.3578\n",
      "\n",
      "Epoch [6/200], CIoU: 0.2758 --- Validation for Epoch [6/200], Validation for Epoch [6], CIoU: 0.3575\n",
      "\n",
      "Epoch [7/200], CIoU: 0.2775 --- Validation for Epoch [7/200], Validation for Epoch [7], CIoU: 0.3584\n",
      "\n",
      "Epoch [8/200], CIoU: 0.2759 --- Validation for Epoch [8/200], Validation for Epoch [8], CIoU: 0.3565\n",
      "\n",
      "Epoch [9/200], CIoU: 0.2766 --- Validation for Epoch [9/200], Validation for Epoch [9], CIoU: 0.3592\n",
      "\n",
      "Epoch [10/200], CIoU: 0.2763 --- Validation for Epoch [10/200], Validation for Epoch [10], CIoU: 0.3553\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[10/200]\n",
      "\n",
      "Epoch [11/200], CIoU: 0.2765 --- Validation for Epoch [11/200], Validation for Epoch [11], CIoU: 0.3550\n",
      "\n",
      "Epoch [12/200], CIoU: 0.2761 --- Validation for Epoch [12/200], Validation for Epoch [12], CIoU: 0.3570\n",
      "\n",
      "Epoch [13/200], CIoU: 0.2763 --- Validation for Epoch [13/200], Validation for Epoch [13], CIoU: 0.3588\n",
      "\n",
      "Epoch [14/200], CIoU: 0.2769 --- Validation for Epoch [14/200], Validation for Epoch [14], CIoU: 0.3579\n",
      "\n",
      "Epoch [15/200], CIoU: 0.2763 --- Validation for Epoch [15/200], Validation for Epoch [15], CIoU: 0.3572\n",
      "\n",
      "Epoch [16/200], CIoU: 0.2757 --- Validation for Epoch [16/200], Validation for Epoch [16], CIoU: 0.3576\n",
      "\n",
      "Epoch [17/200], CIoU: 0.2764 --- Validation for Epoch [17/200], Validation for Epoch [17], CIoU: 0.3575\n",
      "\n",
      "Epoch [18/200], CIoU: 0.2782 --- Validation for Epoch [18/200], Validation for Epoch [18], CIoU: 0.3580\n",
      "\n",
      "Epoch [19/200], CIoU: 0.2768 --- Validation for Epoch [19/200], Validation for Epoch [19], CIoU: 0.3575\n",
      "\n",
      "Epoch [20/200], CIoU: 0.2758 --- Validation for Epoch [20/200], Validation for Epoch [20], CIoU: 0.3579\n",
      "\n",
      "Epoch [21/200], CIoU: 0.2772 --- Validation for Epoch [21/200], Validation for Epoch [21], CIoU: 0.3583\n",
      "\n",
      "Epoch [22/200], CIoU: 0.2759 --- Validation for Epoch [22/200], Validation for Epoch [22], CIoU: 0.3595\n",
      "\n",
      "Epoch [23/200], CIoU: 0.2758 --- Validation for Epoch [23/200], Validation for Epoch [23], CIoU: 0.3590\n",
      "\n",
      "Epoch [24/200], CIoU: 0.2764 --- Validation for Epoch [24/200], Validation for Epoch [24], CIoU: 0.3576\n",
      "\n",
      "Epoch [25/200], CIoU: 0.2763 --- Validation for Epoch [25/200], Validation for Epoch [25], CIoU: 0.3599\n",
      "\n",
      "Epoch [26/200], CIoU: 0.2761 --- Validation for Epoch [26/200], Validation for Epoch [26], CIoU: 0.3556\n",
      "\n",
      "Epoch [27/200], CIoU: 0.2776 --- Validation for Epoch [27/200], Validation for Epoch [27], CIoU: 0.3566\n",
      "\n",
      "Epoch [28/200], CIoU: 0.2761 --- Validation for Epoch [28/200], Validation for Epoch [28], CIoU: 0.3572\n",
      "\n",
      "Epoch [29/200], CIoU: 0.2768 --- Validation for Epoch [29/200], Validation for Epoch [29], CIoU: 0.3592\n",
      "\n",
      "Epoch [30/200], CIoU: 0.2760 --- Validation for Epoch [30/200], Validation for Epoch [30], CIoU: 0.3582\n",
      "Early stopping occurred at 30\n",
      "The best Validation Loss is: 0.35529072023928165\n",
      "********************************************************************************\n",
      "                 Starting Semi-supervised Training - Round 3/23                 \n",
      "********************************************************************************\n",
      "loading checkpoint...\n",
      "................................................................................\n",
      "         Generated Pseudo labels for Unlabelled Dataset (22740 samples)         \n",
      "................................................................................\n",
      "********************************************************************************\n",
      "                            Filtered Top-1000 Label                             \n",
      "********************************************************************************\n",
      "\n",
      "Epoch [1/200], CIoU: 0.3250 --- Validation for Epoch [1/200], Validation for Epoch [1], CIoU: 0.3586\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[1/200]\n",
      "\n",
      "Epoch [2/200], CIoU: 0.3247 --- Validation for Epoch [2/200], Validation for Epoch [2], CIoU: 0.3572\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[2/200]\n",
      "\n",
      "Epoch [3/200], CIoU: 0.3250 --- Validation for Epoch [3/200], Validation for Epoch [3], CIoU: 0.3552\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[3/200]\n",
      "\n",
      "Epoch [4/200], CIoU: 0.3245 --- Validation for Epoch [4/200], Validation for Epoch [4], CIoU: 0.3558\n",
      "\n",
      "Epoch [5/200], CIoU: 0.3248 --- Validation for Epoch [5/200], Validation for Epoch [5], CIoU: 0.3576\n",
      "\n",
      "Epoch [6/200], CIoU: 0.3248 --- Validation for Epoch [6/200], Validation for Epoch [6], CIoU: 0.3597\n",
      "\n",
      "Epoch [7/200], CIoU: 0.3251 --- Validation for Epoch [7/200], Validation for Epoch [7], CIoU: 0.3565\n",
      "\n",
      "Epoch [8/200], CIoU: 0.3252 --- Validation for Epoch [8/200], Validation for Epoch [8], CIoU: 0.3575\n",
      "\n",
      "Epoch [9/200], CIoU: 0.3248 --- Validation for Epoch [9/200], Validation for Epoch [9], CIoU: 0.3560\n",
      "\n",
      "Epoch [10/200], CIoU: 0.3250 --- Validation for Epoch [10/200], Validation for Epoch [10], CIoU: 0.3571\n",
      "\n",
      "Epoch [11/200], CIoU: 0.3244 --- Validation for Epoch [11/200], Validation for Epoch [11], CIoU: 0.3589\n",
      "\n",
      "Epoch [12/200], CIoU: 0.3249 --- Validation for Epoch [12/200], Validation for Epoch [12], CIoU: 0.3556\n",
      "\n",
      "Epoch [13/200], CIoU: 0.3251 --- Validation for Epoch [13/200], Validation for Epoch [13], CIoU: 0.3572\n",
      "\n",
      "Epoch [14/200], CIoU: 0.3248 --- Validation for Epoch [14/200], Validation for Epoch [14], CIoU: 0.3566\n",
      "\n",
      "Epoch [15/200], CIoU: 0.3243 --- Validation for Epoch [15/200], Validation for Epoch [15], CIoU: 0.3559\n",
      "\n",
      "Epoch [16/200], CIoU: 0.3248 --- Validation for Epoch [16/200], Validation for Epoch [16], CIoU: 0.3565\n",
      "\n",
      "Epoch [17/200], CIoU: 0.3248 --- Validation for Epoch [17/200], Validation for Epoch [17], CIoU: 0.3578\n",
      "\n",
      "Epoch [18/200], CIoU: 0.3248 --- Validation for Epoch [18/200], Validation for Epoch [18], CIoU: 0.3598\n",
      "\n",
      "Epoch [19/200], CIoU: 0.3247 --- Validation for Epoch [19/200], Validation for Epoch [19], CIoU: 0.3562\n",
      "\n",
      "Epoch [20/200], CIoU: 0.3246 --- Validation for Epoch [20/200], Validation for Epoch [20], CIoU: 0.3566\n",
      "\n",
      "Epoch [21/200], CIoU: 0.3251 --- Validation for Epoch [21/200], Validation for Epoch [21], CIoU: 0.3565\n",
      "\n",
      "Epoch [22/200], CIoU: 0.3246 --- Validation for Epoch [22/200], Validation for Epoch [22], CIoU: 0.3579\n",
      "\n",
      "Epoch [23/200], CIoU: 0.3249 --- Validation for Epoch [23/200], Validation for Epoch [23], CIoU: 0.3584\n",
      "Early stopping occurred at 23\n",
      "The best Validation Loss is: 0.3552285172045231\n",
      "********************************************************************************\n",
      "                 Starting Semi-supervised Training - Round 4/23                 \n",
      "********************************************************************************\n",
      "loading checkpoint...\n",
      "................................................................................\n",
      "         Generated Pseudo labels for Unlabelled Dataset (21740 samples)         \n",
      "................................................................................\n",
      "********************************************************************************\n",
      "                            Filtered Top-1000 Label                             \n",
      "********************************************************************************\n",
      "\n",
      "Epoch [1/200], CIoU: 0.3675 --- Validation for Epoch [1/200], Validation for Epoch [1], CIoU: 0.3590\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[1/200]\n",
      "\n",
      "Epoch [2/200], CIoU: 0.3676 --- Validation for Epoch [2/200], Validation for Epoch [2], CIoU: 0.3598\n",
      "\n",
      "Epoch [3/200], CIoU: 0.3675 --- Validation for Epoch [3/200], Validation for Epoch [3], CIoU: 0.3573\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[3/200]\n",
      "\n",
      "Epoch [4/200], CIoU: 0.3675 --- Validation for Epoch [4/200], Validation for Epoch [4], CIoU: 0.3554\n",
      "\n",
      "\n",
      "=========Saving Checkpoint======= at Epoch:[4/200]\n",
      "\n",
      "Epoch [5/200], CIoU: 0.3674 --- Validation for Epoch [5/200], Validation for Epoch [5], CIoU: 0.3568\n",
      "\n",
      "Epoch [6/200], CIoU: 0.3674 --- Validation for Epoch [6/200], Validation for Epoch [6], CIoU: 0.3546\n",
      "\n",
      "Epoch [7/200], CIoU: 0.3672 --- Validation for Epoch [7/200], Validation for Epoch [7], CIoU: 0.3563\n",
      "\n",
      "Epoch [8/200], CIoU: 0.3676 --- Validation for Epoch [8/200], Validation for Epoch [8], CIoU: 0.3577\n",
      "\n",
      "Epoch [9/200], CIoU: 0.3674 --- Validation for Epoch [9/200], Validation for Epoch [9], CIoU: 0.3565\n",
      "\n",
      "Epoch [10/200], CIoU: 0.3674 --- Validation for Epoch [10/200], Validation for Epoch [10], CIoU: 0.3569\n",
      "\n",
      "Epoch [11/200], CIoU: 0.3675 --- Validation for Epoch [11/200], Validation for Epoch [11], CIoU: 0.3586\n",
      "\n",
      "Epoch [12/200], CIoU: 0.3675 --- Validation for Epoch [12/200], Validation for Epoch [12], CIoU: 0.3578\n",
      "\n",
      "Epoch [13/200], CIoU: 0.3675 --- Validation for Epoch [13/200], Validation for Epoch [13], CIoU: 0.3596\n",
      "\n",
      "Epoch [14/200], CIoU: 0.3675 --- Validation for Epoch [14/200], Validation for Epoch [14], CIoU: 0.3570\n",
      "\n",
      "Epoch [15/200], CIoU: 0.3676 --- Validation for Epoch [15/200], Validation for Epoch [15], CIoU: 0.3593\n",
      "\n",
      "Epoch [16/200], CIoU: 0.3675 --- Validation for Epoch [16/200], Validation for Epoch [16], CIoU: 0.3557\n",
      "\n",
      "Epoch [17/200], CIoU: 0.3675 --- Validation for Epoch [17/200], Validation for Epoch [17], CIoU: 0.3546\n",
      "\n",
      "Epoch [18/200], CIoU: 0.3675 --- Validation for Epoch [18/200], Validation for Epoch [18], CIoU: 0.3558\n",
      "\n",
      "Epoch [19/200], CIoU: 0.3675 --- Validation for Epoch [19/200], Validation for Epoch [19], CIoU: 0.3555\n",
      "\n",
      "Epoch [20/200], CIoU: 0.3675 --- Validation for Epoch [20/200], Validation for Epoch [20], CIoU: 0.3575\n",
      "\n",
      "Epoch [21/200], CIoU: 0.3677 --- Validation for Epoch [21/200], Validation for Epoch [21], CIoU: 0.3554\n",
      "\n",
      "Epoch [22/200], CIoU: 0.3673 --- Validation for Epoch [22/200], Validation for Epoch [22], CIoU: 0.3575\n",
      "\n",
      "Epoch [23/200], CIoU: 0.3675 --- Validation for Epoch [23/200], Validation for Epoch [23], CIoU: 0.3575\n",
      "\n",
      "Epoch [24/200], CIoU: 0.3675 --- Validation for Epoch [24/200], Validation for Epoch [24], CIoU: 0.3579\n",
      "Early stopping occurred at 24\n",
      "The best Validation Loss is: 0.35540666431188583\n",
      "********************************************************************************\n",
      "                  There is no further improvement observed!!!                   \n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "           Semi-Supervised Learning Piepline is Executed Sucessfully!           \n",
      "********************************************************************************\n",
      "................................................................................\n",
      "          The latest best validation CIoU Loss is: 0.3552285172045231           \n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "best_iter_val_loss = float('inf')\n",
    "max_iterations = 23\n",
    "iteration = 0\n",
    "breaking_point = 0.01\n",
    "iter_break_count = 0\n",
    "patience = 3\n",
    "while iteration < max_iterations:\n",
    "    print_decorated(f\"Starting Semi-supervised Training - Round {iteration+1}/{max_iterations}\", border_char=\"*\")\n",
    "    # load the datasets\n",
    "    csv_filename = rf\"D:\\Praharsha\\code\\CAMZ\\models\\pseudo_labels_semi{iteration}.csv\"\n",
    "    labelled_dataset, unlabeled_dataset = load_datasets(labeled_image_folder, label_subfolder, unlabeled_image_folder)\n",
    "    # load the model\n",
    "    if iteration == 0:\n",
    "        model = load_checkpoint(\"D:\\Praharsha\\code\\CAMZ\\models\\model_history\\CNN_checkpoint.pth.tar\", CNNModel, optimizer)\n",
    "        model = model.to('cuda')\n",
    "    if iteration > 0:\n",
    "         model = load_checkpoint(\"D:\\Praharsha\\code\\CAMZ\\models\\model_history\\CNN_checkpoint_semi.pth.tar\", CNNModel, optimizer)\n",
    "         model = model.to('cuda')\n",
    "    # generate pseudo labels\n",
    "    Generate_pseudo_labels(csv_filename, unlabeled_dataset, model, ciou_loss_function)\n",
    "    if (len(unlabeled_dataset) > 0):\n",
    "        # filter pseudo labels and generate new datasets\n",
    "        filter_top_pseudo_labels(csv_filename, pseudo_labelled_images, pseudo_labels_subfolder, unlabeled_image_folder, num_top=1000)\n",
    "        # prepare the new dataset for training\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "        train_dataset = ZebraFishDataset(labeled_image_folder, pseudo_labelled_images, label_subfolder, pseudo_labels_subfolder, transform=transform)\n",
    "        val_dataset = ZebrafishValDataset(val_image_data, val_labels_subfolder, transform=transform)\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(dataset = val_dataset, batch_size=32, shuffle=True)\n",
    "        # # train the model with new datasets\n",
    "        weighted_ciou_loss_function = WeightedCompleteBoxLoss()\n",
    "        epoch_history_csv_file = f\"D:\\Praharsha\\code\\CAMZ\\models\\Semi_supervised_training_history\\CNN_loss_logger_epoch_wise_semi{iteration}.csv\"\n",
    "        batch_histroy_csv_file = f\"D:\\Praharsha\\code\\CAMZ\\models\\Semi_supervised_training_history\\CNN_loss_logger_batch_wise_semi{iteration}.csv\"\n",
    "        batch_val_history_csv_file = f\"D:\\Praharsha\\code\\CAMZ\\models\\Semi_supervised_training_history\\CNN_loss_logger_val_batch_wise_semi{iteration}.csv\"\n",
    "        model_checkpoint_path = \"D:\\Praharsha\\code\\CAMZ\\models\\model_history\\CNN_checkpoint_semi.pth.tar\"\n",
    "        current_iter_val_loss = train(model, train_loader, val_loader, optimizer, weighted_ciou_loss_function ,save_model_checkpoint_path=model_checkpoint_path,\n",
    "                                      epoch_history_csv_path=epoch_history_csv_file, batch_train_history_csv_path=batch_histroy_csv_file, batch_val_history_csv_path=batch_val_history_csv_file,\n",
    "                                      val_loss_function=ciou_loss_function, num_epochs=200)\n",
    "        \n",
    "        if (best_iter_val_loss - current_iter_val_loss) < breaking_point:\n",
    "            iter_break_count += 1\n",
    "            if iter_break_count == patience:\n",
    "                print_decorated(\"There is no further improvement observed!!!\", border_char='*')\n",
    "                break\n",
    "        best_iter_val_loss = current_iter_val_loss\n",
    "    elif len(unlabeled_dataset) == 0:\n",
    "        print_decorated(\"Unlabelled Dataset is all used up!!!\", '*')\n",
    "        break\n",
    "    iteration += 1\n",
    "print_decorated(f\"Semi-Supervised Learning Piepline is Executed Sucessfully!\", '*')\n",
    "print_decorated(f\"The latest best validation CIoU Loss is: {best_iter_val_loss}\", '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
